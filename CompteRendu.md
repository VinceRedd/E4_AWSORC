# Mod√©lisation

Afin de r√©aliser notre projet de mani√®re efficace, nous avons tout d'abord mis au point cette mod√©lisation qui nous permet d'obtenir une vue d'ensemble de l'architecture √† √©tablir :

![alt text](image-88.png)

_En cas de difficult√©, vous pouvez ouvrir les fichiers images directement, celles-ci sont pr√©sentes dans le r√©pertoire._

# Partie 1 
## üåê Cr√©ation du VPC

Un **VPC (Virtual Private Cloud)** est un r√©seau priv√© isol√© dans le cloud AWS. Il permet de contr√¥ler totalement l'adressage IP, les sous-r√©seaux, les tables de routage et les r√®gles de s√©curit√© de tes ressources AWS.  
C'est l'√©quivalent d'un r√©seau d'entreprise dans le cloud, o√π tu peux d√©ployer des **instances EC2, des bases de donn√©es RDS, des load balancers, etc...** tout en **contr√¥lant l'acc√®s et la communication entre eux**.

---
Nous cr√©ons notre VPC en passant par le **wizard** d'AWS.  
C'est une m√©thode simple et rapide qui configure automatiquement :
- **Deux sous-r√©seaux** (priv√© & public) dans **deux zones de disponibilit√© diff√©rentes** (obligatoire pour RDS).
- Une **passerelle Internet** pour permettre aux ressources publiques d'acc√©der √† Internet.
- Des **r√®gles de s√©curit√©** de base.

L'utilisation du wizard est particuli√®rement pratique, surtout lorsque l'on travaille avec **RDS**, qui exige **au moins deux sous-r√©seaux distincts** pour garantir **la haute disponibilit√©**.

üëâ **Ensuite, on pourra ajuster les param√®tres du VPC selon nos besoins !** 


![alt text](image-20.png)

![alt text](image-23.png)


On peut cr√©er notre VPC.


Ainsi, on obtient :

| Zone de Disponibilit√© | Sous-r√©seau public | Sous-r√©seau priv√© |
|-----------------------|------------------|------------------|
| **us-east-1a** | **E4_CACCIATORE_ProjetFinal-subnet-private1** | **E4_CACCIATORE_ProjetFinal-subnet-private1** |
| us-east-1b | E4_CACCIATORE_ProjetFinal-subnet-public2 | E4_CACCIATORE_ProjetFinal-subnet-private2 |

Ici, on traitera seulement la premi√®re zone de disponibilit√©.
Les tables de routages, passerelle Internet, et connexions r√©seaux sont automatiquement g√©n√©r√©es et g√©r√©es.

## üíª Cr√©ation de l'Instance (Application)

Une **instance EC2 (Elastic Compute Cloud)** est un **serveur virtuel** dans le cloud AWS.  
Elle permet d'ex√©cuter des applications, des sites web ou des services en fonction des besoins.  
EC2 offre une grande **scalabilit√©**, avec la possibilit√© d'ajouter ou de supprimer des instances selon la charge et les performances requises.

---
Ainsi, nous allons mettre en place une **instance EC2** qui h√©bergera notre application et sera accessible depuis l'ext√©rieur.

### üîπ √âtapes de cr√©ation :
1. **S√©lection du VPC**  
   - **VPC** pr√©c√©demment cr√©√© pour s'assurer que l'instance soit bien int√©gr√©e √† notre infrastructure.

2. **Choix du sous-r√©seau**  
   - On choisit le **sous-r√©seau public** pour que l'instance puisse communiquer avec Internet.

3. **Attribution d'une adresse IP publique**  
   - On active **l'attribution automatique d'une adresse IP publique** afin que l'instance soit accessible depuis l'ext√©rieur.

4. **Cr√©ation et configuration du groupe de s√©curit√©**  
   - Ajouter les r√®gles de s√©curit√© n√©cessaires :
     - **SSH (port 22)** : pour l'administration de l'instance (limiter aux IP de confiance).
     - **Port 5085** : on ouvre ce port pour permettre l'acc√®s √† l'application depuis Internet.

üëâ **Une fois l'instance cr√©√©e, nous pourrons d√©ployer notre application et configurer son acc√®s !** 

![alt text](image-24.png)

Pour faciliter l'installation, on a fait le choix d'utiliser un cloud init afin d'installer directement docker et de cloner le r√©pertoire git de notre application (https://github.com/app-generator/flask-datta-able.git).

Voici le code utilis√© :
```
#cloud-config

# Configurer le fuseau horaire
timezone: Europe/Paris

# Mise √† jour des paquets
package_update: true
package_upgrade: true

# Installation des paquets n√©cessaires
packages:
  - git
  - docker
  - docker-compose

# Configuration de l'utilisateur et des groupes
groups:
  - docker
users:
  - default
  - name: ubuntu
    groups: docker
    shell: /bin/bash

runcmd:
  # Activer et d√©marrer Docker
  - systemctl enable --now docker

  # Ajouter l'utilisateur "ubuntu" au groupe Docker
  - usermod -aG docker ubuntu

  # Appliquer les changements imm√©diatement sans red√©marrer (√©vite d'attendre un relog)
  - newgrp docker

  # Cloner l'application Django Volt Dashboard
  - git clone https://github.com/app-generator/flask-datta-able.git /home/ubuntu/flask-datta-able
```
On l'ins√®re tout en bas de notre page de cr√©ation d'instance EC2 :

![alt text](image-25.png)
Apr√®s quelques temps, notre instance est bien cr√©√©e et d√©marr√©e.

## ‚ö° SSH
**SSH (Secure Shell)** est un protocole permettant de se connecter de mani√®re s√©curis√©e √† un serveur distant.  
Il utilise le chiffrement pour garantir la confidentialit√© et l'int√©grit√© des communications entre le client et le serveur.  
C'est l'outil principal pour administrer une instance **EC2** en ligne de commande.

On configure notre environnement VSCode pour qu'on puisse se connecter directement et avoir acc√®s au r√©pertoire distant :

![alt text](image-27.png)

![alt text](image-26.png)

Avant toute chose, il est important d'optimiser le DockerFile :

```
FROM python:3.12-alpine

# set environment variables
ENV PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1 \
    FLASK_APP=run.py

COPY requirements.txt .

# install python dependencies
RUN pip install --no-cache-dir --upgrade pip && \
    pip install --no-cache-dir -r requirements.txt

COPY . .

# Init migration folder
# RUN flask db init # to be executed only once
RUN flask db migrate && flask db upgrade

# gunicorn
CMD ["gunicorn", "--config", "gunicorn-cfg.py", "run:app"]

```

Puis le docker-compose.yml :
```
version: '3.8'
services:
  appseed-app:
    container_name: appseed_app
    restart: always
    build: .
    networks:
      - db_network
      - web_network
    env_file:
        .env
  nginx:
    container_name: nginx
    restart: always
    image: "nginx:1.22.1-alpine-slim"
    ports:
      - "5085:5085"
    volumes:
      - ./nginx:/etc/nginx/conf.d
    networks:
      - web_network
    depends_on: 
      - appseed-app
networks:
  db_network:
    driver: bridge
  web_network:
    driver: bridge
 
```

On optimise aussi le .gitignore :
```
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*.pyo
*.pyd
*.so
*.dll

# Tests and coverage
*.pytest_cache
.coverage
htmlcov/
nosetests.xml
coverage.xml
*.cover
*.py,cover
.cache
junitxml/
*.log
.tox/
.nox/
.envrc

# Database & logs
*.db
#*.sqlite3
*.log
*.sql

# Virtual environments
env/
env_linux/
venv/
venv*/
pip-log.txt
pip-delete-this-directory.txt

# System files
.DS_Store
Thumbs.db
desktop.ini
*.swp
*.swo

# Sphinx docs
_build/
_static/
_templates/

# Node.js / JS dependencies
node_modules/
package-lock.json
yarn.lock
apps/static/assets/node_modules/
apps/static/assets/yarn.lock
apps/static/assets/.temp/

# Migrations (optional, uncomment if you don't want to track migrations)
#migrations/

# IDE and editor files
.vscode/
.vscode/symbols.json
.idea/
*.iml

# Docker related
docker-compose.override.yml
.env
.env.local
.env.production
.env.test
.env.sample
.envrc

# Nginx and deployment-related files
nginx/
*.crt
*.key
*.pem

# Python-specific
__pypackages__/
Pipfile
Pipfile.lock
requirements.txt
pyproject.toml
setup.py
setup.cfg

# Security and auth
*.pem
*.key
*.crt
*.pfx
*.p12

```
Et le .dockerignore :
```
.git
__pycache__
*.pyc
*.pyo
*.pyd
*.db
*.sqlite
*.log
*.log.*
instance/
*.env
*.flaskenv
.vscode
.idea
*.md
package.json
render.yaml
Dockerfile
```

Enfin, on lance l'application : 
```
docker-compose up -d
```
Celle-ci est accessible par d√©faut sur le port 5085 de l'IP publique de notre instance :
![alt text](image-45.png)

On entre donc 44.197.172.233:5085 sur notre machine (h√¥te) dans notre navigateur :
![alt text](image-28.png)
On y a bien acc√®s depuis l'ext√©rieur.

## üóÑ Base de donn√©es (RDS)

On se rend dans le service RDS, puis on cr√©√© notre base de donn√©es MySQL en suivant cette configuration : 

![alt text](image-30.png)

On se met bien sur l'offre gratuite :
![alt text](image-31.png)

On d√©finit nos identifiants :
![alt text](image-32.png)

On active et d√©finit la mise √† l'√©chelle : 
![alt text](image-33.png)

On met en place toute notre connectivit√©.

On cr√©√© un NSG d√©di√© et on s√©lectionne le bon VPC (cr√©√© pr√©c√©demment).
![alt text](image-35.png)

On configure le nom de la DB pour que AWS la cr√©√©e et on active les sauvegardes automatiques : 

![alt text](image-36.png)

On peut d√©sormais la cr√©er.


On configure par la suite une connexion EC2 vers notre instance :
![alt text](image-38.png)


---
On peut d√©sormais se connecter en SSH sur notre DB :
```
mysql -h dbcacciatoretpfinal.csdwaigwo7xn.us-east-1.rds.amazonaws.com -P 3306 -u admin -p
```

![alt text](image-42.png)
![alt text](image-41.png)

On s√©lectionne notre BDD, on regarde nos tables et on affiche l'ensemble des users cr√©√©s. Il n'y en a pas.

On se rend donc sur notre application pour en cr√©er un :

![alt text](image-39.png)

Notre user est bien cr√©√©.
![alt text](image-40.png)

On affiche encore une fois l'ensemble des users cr√©√©s : 
```
SELECT * FROMS users;
```
![alt text](image-43.png)

Notre compte s'affiche bien (un autre "pizza" est aussi pr√©sent car on avait test√© auparavant).

Pour √™tre s√ªr, on en cr√©√© un autre :

![alt text](image-46.png)

![alt text](image-47.png)
On s√©lectionne une nouvelle fois l'ensemble des users dans notre table "users".
Notre user "vincetpfinalAWS" est bien pr√©sent.

## Cr√©ation d'une seconde instance (serveur de test)

On cr√©√© une seconde instance Ubuntu dans notre VPC dans le priv√© :

_E4_CACCIATORE_ProjetFinal_ServTestPrive_

![alt text](image-48.png)

On cr√©√© par la suite notre passerelle NAT.
Elle permet aux instances d‚Äôun sous-r√©seau priv√© d‚Äôacc√©der √† Internet tout en emp√™chant les connexions entrantes. Elle est utile pour t√©l√©charger des mises √† jour ou envoyer des requ√™tes sans exposer directement les instances √† Internet.

![alt text](image-49.png)

Enfin, depuis notre premi√®re instance (publique), on se connecte √† la seconde via son ip priv√©e :
![alt text](image-51.png)
![alt text](image-52.png)

On arrive bien √† ping vers les serveurs de Google depuis notre instance en s'ayant connect√©e via l'IP priv√©e au travers de notre premi√®re instance "publique" :
![alt text](image-53.png)
![alt text](image-54.png)

# üì¶ Bucket

Un **bucket S3** est un conteneur de stockage dans **Amazon S3**, permettant de **stocker et organiser des fichiers** (objets) dans le cloud. Il est utilis√© pour **h√©berger des donn√©es** comme des images, vid√©os, logs ou sauvegardes, avec une **haute disponibilit√©** et une **s√©curit√© renforc√©e**.  

On cr√©√© notre compartiment en suivant cette configuration :

![alt text](image-57.png)
![alt text](image-56.png)

On se rend dans l'onglet "Autorisations" de notre compartiment :
![alt text](image-59.png)

![alt text](image-60.png)

Tout en bas de "Propri√©t√©s" :
![alt text](image-65.png)

![alt text](image-66.png)

On glisse notre site web statique :
![alt text](image-67.png)

On a bien notre site web statique :
![alt text](image-68.png)

![alt text](image-69.png)

On a bien acc√®s √† notre site depuis Internet :

![alt text](image-70.png)


## üñ¥ Sauvegardes

### EC2

Dans le menu de l'instance EC2 :

![alt text](image-71.png)

![alt text](image-72.png)

On vient de cr√©er une snapshot de notre instance.

### DataBase

De la m√™me fa√ßon, on se rend dans l'interface RDS :
![alt text](image-73.png)


![alt text](image-74.png)

On a bien nos snapshots de notre DB et notre application :
![alt text](image-75.png)
![alt text](image-76.png)


# Partie 2

On cr√©√© le VPC avec la bonne configuration :
![alt text](image-77.png)

On cr√©√© le sous-r√©seau : 
![alt text](image-78.png)

On cr√©√© la table de routage :
![alt text](image-79.png)

On associe notre subnet √† notre RTB :
![alt text](image-80.png)

On met en place ensuite le peering entre nos deux VPCs.
Le **peering** permet de **connecter directement** deux r√©seaux, comme **deux VPC sur AWS**, sans passer par Internet.
Id√©al pour **interconnecter** des services tout en gardant un **trafic priv√©** et **optimis√©**.

![alt text](image-81.png)
![alt text](image-82.png)

RTB de mon VPC2 :
![alt text](image-83.png)

RTB de mon VPC1 Subnet Public :
![alt text](image-84.png)

---
On cr√©√© une autre instance EC2 que l'on met dans notre second VPC : 
![alt text](image-85.png)

On arrive bien √† se connecter √† l'instance pr√©sente dans le VPC2, depuis l'instance pr√©sente dans le subnet publique du VPC1.

![alt text](image-86.png)

On teste le ping (on a autoris√© le protocole ICMP sur mon NSG) : 

![alt text](image-87.png)

On a bien acc√®s √† Internet.

### üìå Conclusion

Dans le cadre de ce projet, nous avons mis en place une **infrastructure cloud robuste et s√©curis√©e** sur **AWS**, en respectant les exigences du client tout en optimisant les co√ªts et la scalabilit√©.  

---

### üîπ **Partie 1 : D√©ploiement de l'environnement initial**  
Nous avons construit un **Virtual Private Cloud (VPC)** avec :  
‚úîÔ∏è **Un sous-r√©seau public** h√©bergeant une **instance EC2** pour l‚Äôapplication web.  
‚úîÔ∏è **Un sous-r√©seau priv√©** contenant un **serveur de test** non accessible directement depuis Internet.  
‚úîÔ∏è Une **base de donn√©es RDS** s√©curis√©e et scalable, connect√©e √† l‚Äôapplication.  
‚úîÔ∏è Un **bucket S3** pour l‚Äôh√©bergement **statique des fichiers**.  
‚úîÔ∏è Une **passerelle NAT** permettant au serveur de test d‚Äôacc√©der √† Internet sans √™tre expos√©.  
‚úîÔ∏è Des **sauvegardes automatis√©es** (snapshots EC2 & RDS) pour assurer la r√©silience du syst√®me.  

Le d√©ploiement de l‚Äôapplication s‚Äôest fait via **Docker** et **Nginx**, permettant une **gestion efficace des conteneurs** et une meilleure modularit√©. La connexion entre l‚Äôapplication et la base de donn√©es a √©t√© v√©rifi√©e via des tests fonctionnels, garantissant le bon √©change des donn√©es.  

---

### üîπ **Partie 2 : Ajout d‚Äôun second environnement pour l‚ÄôIA**  
Le client souhaitant scinder ses environnements, nous avons mis en place :  
‚úîÔ∏è **Un deuxi√®me VPC**, isol√© du premier, contenant un serveur pour l‚Äô√©quipe IA.  
‚úîÔ∏è **Un peering entre les VPCs** pour permettre la communication s√©curis√©e entre les deux environnements.  
‚úîÔ∏è Une gestion des acc√®s et des r√®gles de routage optimis√©es, garantissant un **trafic s√©curis√©** et un acc√®s √† Internet sans exposition.  

---

### üöÄ **Bilan et perspectives**  

Avec cette architecture, l‚Äôentreprise dispose d‚Äôun **environnement de d√©veloppement performant**, pr√™t √† √©voluer vers un **d√©ploiement en production s√©curis√© et extensible**. üöÄüîí
